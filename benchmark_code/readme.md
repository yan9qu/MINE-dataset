The benchmark code for MINE:Multimodal IntentioN and Emotion Understanding in the Wild.
---
Contains "MulT"\[1\], "MISA"\[2\] and "MPMM"\[3\].

Usage: python run.py --method [method_name]


Most of the code modified from MintRec \[4\], thanks!

\[1\] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov, “Multimodal transformer for unaligned multimodal language sequences,” in Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics. 2019, pp. 6558–6569.

\[2\]D. Hazarika, R. Zimmermann, and S. Poria, “Misa: Modality-invariant and-specific representations for multimodal sentiment analysis,” in Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 1122–1131

\[3\]Y.-L. Lee, Y.-H. Tsai, W.-C. Chiu, and C.-Y. Lee, “Multimodal prompting with missing modalities for visual recognition,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14 943–14 952

\[4\] H. Zhang, H. Xu, X. Wang, Q. Zhou, S. Zhao, and J. Teng, “Mintrec: A new dataset for multimodal intent recognition,” in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 1688–1697
